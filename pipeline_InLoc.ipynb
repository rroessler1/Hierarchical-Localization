{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "from hloc import extract_features, match_features, localize_inloc, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for indoor localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we declare the paths to the dataset, image pairs, and we choose the feature extractor and the matcher. You need to download the [InLoc dataset](https://www.visuallocalization.net/datasets/) and put it in `datasets/inloc/`, or change the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Path(\"datasets/inloc/\")  # change this if your dataset is somewhere else\n",
    "\n",
    "pairs = Path(\"pairs/inloc/\")\n",
    "loc_pairs = pairs / \"pairs-ross300.txt\"  # top 40 retrieved by NetVLAD\n",
    "\n",
    "outputs = Path(\"outputs/inloc/\")  # where everything will be saved\n",
    "results = outputs / \"InLoc_hloc_superpoint+superglue_netvlad40.txt\"  # the result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs for feature extractors:\n",
      "{'d2net-ss': {'model': {'multiscale': False, 'name': 'd2net'},\n",
      "              'output': 'feats-d2net-ss',\n",
      "              'preprocessing': {'grayscale': False, 'resize_max': 1600}},\n",
      " 'dir': {'model': {'name': 'dir'},\n",
      "         'output': 'global-feats-dir',\n",
      "         'preprocessing': {'resize_max': 1024}},\n",
      " 'disk': {'model': {'max_keypoints': 5000, 'name': 'disk'},\n",
      "          'output': 'feats-disk',\n",
      "          'preprocessing': {'grayscale': False, 'resize_max': 1600}},\n",
      " 'eigenplaces': {'model': {'name': 'eigenplaces'},\n",
      "                 'output': 'global-feats-eigenplaces',\n",
      "                 'preprocessing': {'resize_max': 1024}},\n",
      " 'netvlad': {'model': {'name': 'netvlad'},\n",
      "             'output': 'global-feats-netvlad',\n",
      "             'preprocessing': {'resize_max': 1024}},\n",
      " 'openibl': {'model': {'name': 'openibl'},\n",
      "             'output': 'global-feats-openibl',\n",
      "             'preprocessing': {'resize_max': 1024}},\n",
      " 'r2d2': {'model': {'max_keypoints': 5000, 'name': 'r2d2'},\n",
      "          'output': 'feats-r2d2-n5000-r1024',\n",
      "          'preprocessing': {'grayscale': False, 'resize_max': 1024}},\n",
      " 'sift': {'model': {'name': 'dog'},\n",
      "          'output': 'feats-sift',\n",
      "          'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'sosnet': {'model': {'descriptor': 'sosnet', 'name': 'dog'},\n",
      "            'output': 'feats-sosnet',\n",
      "            'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_aachen': {'model': {'max_keypoints': 4096,\n",
      "                                 'name': 'superpoint',\n",
      "                                 'nms_radius': 3},\n",
      "                       'output': 'feats-superpoint-n4096-r1024',\n",
      "                       'preprocessing': {'grayscale': True,\n",
      "                                         'resize_max': 1024}},\n",
      " 'superpoint_inloc': {'model': {'max_keypoints': 4096,\n",
      "                                'name': 'superpoint',\n",
      "                                'nms_radius': 4},\n",
      "                      'output': 'feats-superpoint-n4096-r1600',\n",
      "                      'preprocessing': {'grayscale': True, 'resize_max': 1600}},\n",
      " 'superpoint_max': {'model': {'max_keypoints': 4096,\n",
      "                              'name': 'superpoint',\n",
      "                              'nms_radius': 3},\n",
      "                    'output': 'feats-superpoint-n4096-rmax1600',\n",
      "                    'preprocessing': {'grayscale': True,\n",
      "                                      'resize_force': True,\n",
      "                                      'resize_max': 1600}}}\n",
      "Configs for feature matchers:\n",
      "{'NN-mutual': {'model': {'do_mutual_check': True, 'name': 'nearest_neighbor'},\n",
      "               'output': 'matches-NN-mutual'},\n",
      " 'NN-ratio': {'model': {'do_mutual_check': True,\n",
      "                        'name': 'nearest_neighbor',\n",
      "                        'ratio_threshold': 0.8},\n",
      "              'output': 'matches-NN-mutual-ratio.8'},\n",
      " 'NN-superpoint': {'model': {'distance_threshold': 0.7,\n",
      "                             'do_mutual_check': True,\n",
      "                             'name': 'nearest_neighbor'},\n",
      "                   'output': 'matches-NN-mutual-dist.7'},\n",
      " 'adalam': {'model': {'name': 'adalam'}, 'output': 'matches-adalam'},\n",
      " 'disk+lightglue': {'model': {'features': 'disk', 'name': 'lightglue'},\n",
      "                    'output': 'matches-disk-lightglue'},\n",
      " 'superglue': {'model': {'name': 'superglue',\n",
      "                         'sinkhorn_iterations': 50,\n",
      "                         'weights': 'outdoor'},\n",
      "               'output': 'matches-superglue'},\n",
      " 'superglue-fast': {'model': {'name': 'superglue',\n",
      "                              'sinkhorn_iterations': 5,\n",
      "                              'weights': 'outdoor'},\n",
      "                    'output': 'matches-superglue-it5'},\n",
      " 'superpoint+lightglue': {'model': {'features': 'superpoint',\n",
      "                                    'name': 'lightglue'},\n",
      "                          'output': 'matches-superpoint-lightglue'}}\n"
     ]
    }
   ],
   "source": [
    "# list the standard configurations available\n",
    "print(f\"Configs for feature extractors:\\n{pformat(extract_features.confs)}\")\n",
    "print(f\"Configs for feature matchers:\\n{pformat(match_features.confs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one of the configurations for extraction and matching\n",
    "# you can also simply write your own here!\n",
    "feature_conf = extract_features.confs[\"superpoint_inloc\"]\n",
    "matcher_conf = match_features.confs[\"superglue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract local features for database and query images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024/10/29 16:47:21 hloc INFO] Extracting local features with configuration:\n",
      "{'model': {'name': 'netvlad'},\n",
      " 'output': 'global-feats-netvlad',\n",
      " 'preprocessing': {'resize_max': 1024}}\n",
      "[2024/10/29 16:47:21 hloc INFO] Found 7119 images in root datasets/lamar.\n",
      "[2024/10/29 16:47:23 hloc INFO] Skipping the extraction.\n"
     ]
    }
   ],
   "source": [
    "feature_path = extract_features.main(feature_conf, dataset, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the query images\n",
    "Here we assume that the localization pairs are already computed using image retrieval (NetVLAD). To generate new pairs from your own global descriptors, have a look at `hloc/pairs_from_retrieval.py`. These pairs are also used for the localization - see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024/10/29 16:42:28 hloc INFO] Matching local features with configuration:\n",
      "{'model': {'name': 'superglue',\n",
      "           'sinkhorn_iterations': 50,\n",
      "           'weights': 'outdoor'},\n",
      " 'output': 'matches-superglue'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                        | 0/123480 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing key keypoints0 in data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m match_path \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatcher_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_conf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/Hierarchical-Localization/hloc/match_features.py:178\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(conf, pairs, features, export_dir, matches, features_ref, overwrite)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features_ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     features_ref \u001b[38;5;241m=\u001b[39m features_q\n\u001b[0;32m--> 178\u001b[0m \u001b[43mmatch_from_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/venv2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/Hierarchical-Localization/hloc/match_features.py:248\u001b[0m, in \u001b[0;36mmatch_from_paths\u001b[0;34m(conf, pairs_path, match_path, feature_path_q, feature_path_ref, overwrite)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(loader, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)):\n\u001b[1;32m    244\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    245\u001b[0m         k: v \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m v\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    247\u001b[0m     }\n\u001b[0;32m--> 248\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     pair \u001b[38;5;241m=\u001b[39m names_to_pair(\u001b[38;5;241m*\u001b[39mpairs[idx])\n\u001b[1;32m    250\u001b[0m     writer_queue\u001b[38;5;241m.\u001b[39mput((pair, pred))\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/venv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/Hierarchical-Localization/hloc/utils/base_model.py:24\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the data and call the _forward method of the child model.\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequired_inputs:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m in data\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(data)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing key keypoints0 in data"
     ]
    }
   ],
   "source": [
    "match_path = match_features.main(\n",
    "    matcher_conf, loc_pairs, feature_conf[\"output\"], outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize!\n",
    "Perform hierarchical localization using the precomputed retrieval and matches. Different from when localizing with Aachen, here we do not need a 3D SfM model here: the dataset already has 3D lidar scans. The file `InLoc_hloc_superpoint+superglue_netvlad40.txt` will contain the estimated query poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024/10/28 17:38:06 hloc INFO] Starting localization...\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:39<00:00,  2.63s/it]\n",
      "[2024/10/28 17:38:45 hloc INFO] Writing poses to outputs/inloc/InLoc_hloc_superpoint+superglue_netvlad40.txt...\n",
      "[2024/10/28 17:38:45 hloc INFO] Writing logs to outputs/inloc/InLoc_hloc_superpoint+superglue_netvlad40.txt_logs.pkl...\n",
      "[2024/10/28 17:38:45 hloc INFO] Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't localize query/iphone7/IMG_0994.JPG\n",
      "Couldn't localize query/iphone7/IMG_1107.JPG\n"
     ]
    }
   ],
   "source": [
    "localize_inloc.main(\n",
    "    dataset, loc_pairs, feature_path, match_path, results, skip_matches=20\n",
    ")  # skip database images with too few matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "We parse the localization logs and for each query image plot matches and inliers with a few database images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/inloc/InLoc_hloc_superpoint+superglue_netvlad40.txt_logs.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ross/Daten/programming_data/mixed_reality/Hierarchical-Localization/hloc/visualization.py:79\u001b[0m, in \u001b[0;36mvisualize_loc\u001b[0;34m(results, image_dir, reconstruction, db_image_dir, selected, n, seed, prefix, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_loc\u001b[39m(\n\u001b[1;32m     67\u001b[0m     results,\n\u001b[1;32m     68\u001b[0m     image_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     76\u001b[0m ):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m image_dir\u001b[38;5;241m.\u001b[39mexists()\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_logs.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         logs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m selected:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/inloc/InLoc_hloc_superpoint+superglue_netvlad40.txt_logs.pkl'"
     ]
    }
   ],
   "source": [
    "visualization.visualize_loc(results, dataset, n=2, top_k_db=2, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
